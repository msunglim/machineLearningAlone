https://colab.research.google.com/?hl=ko

접속 취소하고 
연결 ->  호스팅된 런타임연결

컨트롤 엔터로 실행
알트 엔터로 실행 후 아래줄이동
시프트 엔터로 아랫줄로이동

구글드라이브나 github에 저장가능'

사용하게될 라이브러리는 sklearn , tensorflow다.

런타임은 평소에는 none이지만 tensorflow쓸땐 GPU로 설정.

1. 그래프만들기
import matplotlib.pyplot as plt

bream_length = [25.4,26.3,26.5,29.0,29.0,29.7,29.7,30.0,30.7,30.9,31.0,31.0,31.5,32.0,32.0,33.0,33.5,33.5,34.0,34.0]
# print(len(bream_length))
bream_weight= [430.3,450.0,500.0,390.0,450.0,500.0,475.0,500.0,500.0,340.0,600.0,600.0,700.0,700.0,610.0,650.0,575.0,685.0,620.0,680.0]

plt.scatter(bream_length, bream_weight)
plt.xlabel('length')
plt.ylabel('weight')

plt.show()

신기하다. 원리는 plt.scatter(X축, Y축) 과 show

하나의 그래프에 여러개의 데이터를 넣을 수 있다.
scatter( data1, data1_)
scatter(data2, data2_) 

2. 데이터합치기

length = bream_length + smelt_length
weight = bream_weight+ smelt_weight

fish_data =[[l,w] for l, w in zip(length, weight)]

이렇게하면 [  [l1, w1].... [ln, wn]]  2 dimensional array가 만들어져.

이렇게합쳐놓고, 뭐가 뭔지 지정을 해줘야해. 

fish_target = [1]*len(bream_length) + [0]*len(smelt_length) 이렇게하면 bream은 1로써 저장되고 smelt는 0으로써 저장되겠지. 찾을려는 정보를 1로 놓는게 좋다. 

***numpy의 기능을 이용해 간단하게 표현할 수 있다.***
# fish_data =[[l,w] for l, w in zip(length, weight)]
fish_data = np.column_stack((length, weight))
 col1에 길이 데이터, col2에 무게 데이터를 넣는다는뜻.

# fish_target = [1]*len(bream_length) + [0]*len(smelt_length)
fish_target = np.concatenate((np.ones(len(bream_length)), np.zeros(len(smelt_length))))

np.ones/zeros(N)이면 N개의 1로 이루어진 arr를 생성
np.concatenate(arr1, arr2)는 arr1과 arr2를 합침

3. K-최근접이웃
from sklearn.neighbors import KNeighborsClassifier

kn = KNeighborsClassifier()

//machine learning model
kn.fit(fish_data, fish_target)   

//그냥 얼마나 잘맞추는지. 1= 100%정답률.
kn.score(fish_data, fish_target)

 새로운 데이터를 주고 어느 집단에속하는지 알게하는법

kn.predict([[2, 10]])
이렇게주면 0이나오고 [[30,600  ]] 이런걸로주면 1로나옴.! 가장 가까운 이웃들이 어떤 데이터인지 보고 주어진 데이터의 값도 이웃들과 비슷하겠거니함.. 


4. 정확도
kn.predict([[30, 500]])

kn49 = KNeighborsClassifier(n_neighbors = (len(bream_length)+len(smelt_length)))
kn49.fit(fish_data, fish_target)
kn49.score(fish_data, fish_target)

이것은 58퍼센트의 확률로 bream이나옴. 왜냐면 bream데이타수/총 데이터수 = 0.58이기때문에. 위에꺼랑다른것은, ()일경우에는 바라보는데이터가 정의되지않아서..? 1:1로 대응하면 전부 자기랑 정답이니까.. 근데 2번째거처럼 모든데이타를 바라보면, 1인건 정답이지만 0인건 오답이라..?

5. train/test set

train_input/target과 test_input/target을 분리한다.
연습할때는 train, 실험해볼때는 test로 해본다.
train_input = fish_data[:len(A집단)]
train_target =fish_target[: len(A집단)]

test_input = fish_data[len(A집단):]
test_target = fish_target[len(A집단):]

kn = KNeighborsClassifier()
kn = kn.fit(train_input, train_target)

kn.score(test_input, test_target)

대신 이렇게하면, train할때와 test할때 데이터가 딴판일수 있기때문에,

numpy를 이용해야한다.

import numpy as np

input_arr = np.array(fish_data)
target_arr = np.array(fish_target)

np.array하지않았을때는 [ [] , [] , [] ] 의 모양을 가지지만
np.array를하면 쉼표가 사라지고
[
[]
[]
[]
]의 모습을 가진다
 
index = np.arange(len(A+B집단))
np.random.shuffle(index)



이것을 해준후,
train_input = input_arr[index[: len(A집단)]]
train_target = target_arr[index[:len(A집단)]]

test_input = input_arr[index[len(A집단): ]]
test_target = target_arr[index[len(A집단): ]]

이것들은 배열 슬라이싱인데, input_arr 의 index[0:a집단의수]면 (index에있는 0~집단의 수th 번째 원소에 해당하는 값)번째에 있는 input_arr의 원소들을 반환해

예를 들면
a= np.array([5,6,7,8])
print(a[[1,3]]) 이거하면, 6,8이 불려나와.

이렇게한것을
plt.scatter(train_input[:,0], train_input[:, 1])
plt.scatter(test_input[:,0], test_input[:,1])
plt.xlabel('length')
plt.ylabel('width')
plt.show()
로 테스트해볼수 있는데, 이러면 train과 test가 잘 섞인것을 볼 수 있어.

여기서 train_input[: ,0]이런게 있을텐데  [a:b, c:d]라고하면 a~b까지 row, c~d까지의 col을 선택하는건데 예제에서는 , 앞에는 0~모두라 생략함. 그리고 ,뒤에는 0번째 col만 선택하는건데, 이번예제에서는 [[길이, 넓이], [길이, 넓이]...] 이런식으로 약속이 되있기때문에, 길이만 전체선택한다고 볼 수 있다.

: 는 모두를 의미
, 앞의 수는 row을 의미
, 뒤의 수는 col을 의미

따라서 모든 1번째 col선택은 arr[ :, 1]이 되겠다.

6. numpy와 sklearn으로 세련된 코딩하기
***numpy의 기능을 이용해 간단하게 표현할 수 있다.***
# fish_data =[[l,w] for l, w in zip(length, weight)]
fish_data = np.column_stack((length, weight))
 col1에 길이 데이터, col2에 무게 데이터를 넣는다는뜻.

# fish_target = [1]*len(bream_length) + [0]*len(smelt_length)
fish_target = np.concatenate((np.ones(len(bream_length)), np.zeros(len(smelt_length))))

np.ones/zeros(N)이면 N개의 1로 이루어진 arr를 생성
np.concatenate(arr1, arr2)는 arr1과 arr2를 합침

ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ
from sklearn.model_selection import train_test_split


train_input, test_input, train_target, test_target = train_test_split(
    fish_data, fish_target, stratify= fish_target, random_state=len(fish_data))

이렇게해주면, 
train_input과 test_input 이 나뉘고 train_target과 test_target이 나뉜다.
stratify가 fish_target의 데이터를 보고 그것들이 골고루 섞일 수 있도록 섞는다. 그래서 1,0밖에없는 target이 골고루 섞인상태가되고 마지막 random_state로 어느 한기점을 중심으로 반토막내버려. 그 반토막을 기준으로 train과 test 집단이 나뉘게 되는데, 자연스래 train과 test 값들이 골고루 쪼개질 수 밖에없다.


입력데이터로부터 가까운 이웃까지 거리와 가까운 이웃의 좌표를 반환한다.
이때 가까운 이웃은 kneighbor에 기반한다. kneighbor의 기본값은 5이다.
#returns distance from point to its neighbors and indexes of neighbors based on Kn_neighbors 
distances, indexes = kn.kneighbors([[25,150]])
# print(distances) index is generated based on the graph.
# print(indexes)
[[130.48375378 138.37485321 140.64938677 140.72046759 140.81068851]]
[[ 3 23 16 22 21]] 이런 형식으로.


plt.scatter(train_input[:,0], train_input[:, 1])
# mark triangle on input point 삼각형으로 입력값의 좌표를 표시한다.
plt.scatter(25,150, marker= '^')

# mark diamond on points close to input point 다이아몬드로 가까운이웃을 표시한다.
plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')

이렇게했을때, 육안으로 봣을때 가까운 이웃들이 선택되지않았다면 그것은 아마 그래프 스케일의 문제일 것이다. 예를들어 x: 0~10 이고 y:0~1000이라면, 육안으로 비율이 같게 정사각형모양의 그래프에서는 5,500은 6,700 이 더 가까워보일 수 있지만 실제로는 1,400이 더가깝다. 스케일 조정으로 이런 치사한 오류를 피할 수 있다. 

plt.xlim( min, max) 로 y축에맞게 x축을 조절해보자. 그럼 왜 이런일이 있는지 알 수 있다.

표준점수로 바꿔 이 문제를 해결해보자.
mean = np.mean(train_input, axis=0)
std = np.std(train_input, axis=0)

train_scaled = (train_input - mean)/std

mean의 크기는 train_input과 같기때문에 바로위의 코드를 실행하면 모든 train_iput에서 mean의 값을 빼주고 std로 나눠준다.

test_input의 값들 또한 평균/표준편차를 이용해 scale을 맞춰준다.


kn = kn.fit(train_scaled, train_target)
test_scaled = (test_input - mean)/std
kn.score(test_scaled, test_target)

입력값또한 scale을 맞춰준다.

new = ([25,150]- mean)/std
distances, indexes = kn.kneighbors([new])

그리고 맞춰진 애들을 가지고 새로운 그래프를 그려보자

plt.scatter(train_scaled[:,0], train_scaled[:, 1])
# mark triangle on input point
plt.scatter(new[0], new[1], marker = '^')
# mark diamond on points close to input point 
plt.scatter(train_scaled[indexes,0], train_scaled[indexes,1], marker='D')

plt.xlabel('length')
plt.ylabel('weight')
plt.show()

이렇게하면 육안으로도 공감/이해할 수있는 이웃들이 다이아몬드로 마크된다. 
이렇게 스케일을 맞춰주는 작업을 전처리 작업이라고한다. tree에서는 할 필요가 없다고한다.

7. 회귀(regression)에는 target값을 맞출 필요가없다.예측값이기때문에 이미있는 값(위에서 target [1,0])이아니라 새로운값을 예측하기때문에. 
왜 회귀라고 부르냐면 콜턴이라는 애가 19세기에 키 큰 부모의 자식이 그정도로 키가 크기않고 동년배 평균값의 키로 회귀하드라 라고 예측을 한 논문에서 regression이라는 단어가 굳어졌다...

위의 방식 (최근접분류)과 회귀가 다른것은 최근접분류는 가까운 이웃이 ㅁ,ㅁ,ㅅ이면 타겟의 값은 ㅁ으로 분류하는거고
최근접회귀는  만약 가까운이웃이 10 20 30 이라는 값을 가지고 있다면 [10+20+30]/ 3 = 20 의 값으로 예측하는거다. 

8. 회귀문제에선 stratify 가 필요없다.
#we don't need stratify for regression.
train_input, test_input, train_target, test_target = train_test_split(
    perch_length, perch_weight, random_state= 20
)
이러면 perch_length를 train과 test input이 나눠먹고, perch_weight를 train과 test target이 나눠먹어.

이러면 모양이 [ [v1,v2] , [v3,v4], .....]이런모양이나오기때문에, 
[
[v1,v2],
[v3,v4]
...
]
이렇게 하고싶다면 reshape를 써라.

train_input = np.array(train_input).reshape(-1,1)
test_input = np.array(test_input).reshape(-1,1)

reshape( 줄 갯수,1 set당 원소의 갯수)
줄 갯수가 -1일경우, 알아서 해라라는뜻? 

from sklearn.neighbors import KNeighborsRegressor

knr = KNeighborsRegressor()
knr.fit(train_input, train_target)

knr.score(test_input, test_target)
이걸로 정확도 테스트를 할 수 있는데, 그 공식은 R^2 = 1 - (sum of (target-input)^2 )/(sum of(target-mean)^2) 라고한다. 식을보면, 분수중 위의 target-input값이 0이면, 뺄셈의 값이 1이기때문에 매우 정확하게 된다는걸 알수 있고, 분자와 분모의 값이 같아질수록 뺄셈의 값이 0이기때문에 정확도가 떨어진다는것을 알수 있다. 다시말하면 예측값이 평균과 같아질수록 정확도는 떨어진다..?ㄷㄷ

9. 오차
아래코드는 오차값을 나타내준다.
from sklearn.metrics import mean_absolute_error
test_prediction = knr.predict(test_input)
mae = mean_absolute_error(test_target, test_prediction)

결과)
[[33.5]  print(test_input)
 [32. ]
 [31. ]
 [29. ]
 [26.3]]
[675.0, 615.0, 600.0, 450.0, 450.0]  print(test_target)
[649.   602.   550.   493.   471.06] print(test_prediction)
30.612000000000013 print(mae)


예제들에서 input = 길이 target=무게 였던것을 상기하자.

코드를보면 test_prediction은 test_input (length)값에 기초해서 weight를 예측한다. 그것이 실제 target (weight)값과 비교했을때 오차가 좀 있는데, 그것이 mae이다. 

10. 과대적합/ 과소적합
print("train score:",knr.score(train_input, train_target) , "\ntest score",knr.score(test_input, test_target))
과소적합 (underfitting): train score < test score. 연습할때보다 실제시험에서 더 잘맞춤
과대적합 (overfitting): train score > test score.  연습할땐 잘했는데 실제시험 조짐

해소법은 knr.n_neighbors를 조절하면된다. 기본값이 5인데, 그것보다 높으면 과소적합, 낮으면 과대적합 
k가 높아질수록 train때보다 test값이 커진다. 
k가 낮아질수록 train때보다 test값이 작아진다.