https://colab.research.google.com/?hl=ko

접속 취소하고 
연결 ->  호스팅된 런타임연결

컨트롤 엔터로 실행
알트 엔터로 실행 후 아래줄이동
시프트 엔터로 아랫줄로이동

구글드라이브나 github에 저장가능'

사용하게될 라이브러리는 sklearn , tensorflow다.

런타임은 평소에는 none이지만 tensorflow쓸땐 GPU로 설정.

1. 그래프만들기
import matplotlib.pyplot as plt

bream_length = [25.4,26.3,26.5,29.0,29.0,29.7,29.7,30.0,30.7,30.9,31.0,31.0,31.5,32.0,32.0,33.0,33.5,33.5,34.0,34.0]
# print(len(bream_length))
bream_weight= [430.3,450.0,500.0,390.0,450.0,500.0,475.0,500.0,500.0,340.0,600.0,600.0,700.0,700.0,610.0,650.0,575.0,685.0,620.0,680.0]

plt.scatter(bream_length, bream_weight)
plt.xlabel('length')
plt.ylabel('weight')

plt.show()

신기하다. 원리는 plt.scatter(X축, Y축) 과 show

하나의 그래프에 여러개의 데이터를 넣을 수 있다. 그럼 알아서 색깔도 다르게해줌
plt.scatter( data1, data1_)
plt.scatter(data2, data2_) 

2. 데이터합치기 for 분류 (Classifier)

length = bream_length + smelt_length
weight = bream_weight+ smelt_weight

fish_data =[[l,w] for l, w in zip(length, weight)]

이렇게하면 [  [l1, w1].... [ln, wn]]  2 dimensional array가 만들어져.

이렇게합쳐놓고, 뭐가 뭔지 지정을 해줘야해. (정답지)

fish_target = [1]*len(bream_length) + [0]*len(smelt_length) 이렇게하면 bream은 1로써 저장되고 smelt는 0으로써 저장되겠지. 찾을려는 정보를 1로 놓는게 좋다. 
[1,1,1,1,1,1,1,1,0,0,0,0,0,0,0] 이런모양이 돼.

***numpy의 기능을 이용해 간단하게 표현할 수 있다.***
# fish_data =[[l,w] for l, w in zip(length, weight)]
fish_data = np.column_stack((length, weight))
 col1에 길이 데이터, col2에 무게 데이터를 넣는다는뜻.
[ [ l1, w1 ], 
   [l2, w2]...] 이렇게 저모양으로 스텍이쌓이네요


# fish_target = [1]*len(bream_length) + [0]*len(smelt_length)
fish_target = np.concatenate((np.ones(len(bream_length)), np.zeros(len(smelt_length))))

np.ones/zeros(N)이면 N개의 1로 이루어진 arr를 생성
np.concatenate(arr1, arr2)는 arr1과 arr2를 합침

3. K-최근접이웃
from sklearn.neighbors import KNeighborsClassifier

kn = KNeighborsClassifier()

//machine learning model
kn.fit(fish_data, fish_target)   
반반나눈 data와 target을 묶는다.. data값일때 정답은 target이게.. 

//그냥 얼마나 잘맞추는지. 1= 100%정답률. = 여기선 당연히 그래야하는게 섞을때 같은 index로했기때문에.. 예를들어 data[n]가 '대한민국'였을경우 target[n]='Korea'이기때문에 항상일치.
kn.score(fish_data, fish_target)

 새로운 데이터를 주고 어느 집단에속하는지 알게하는법

kn.predict([[2, 10]])
이렇게주면 0이나오고 [[30,600  ]] 이런걸로주면 1로나옴.! 가장 가까운 이웃들이 어떤 데이터인지 보고 주어진 데이터의 값도 이웃들과 비슷하겠거니함.. 


4. 정확도
kn.predict([[30, 500]])

kn49 = KNeighborsClassifier(n_neighbors = (len(bream_length)+len(smelt_length)))
kn49.fit(fish_data, fish_target)
kn49.score(fish_data, fish_target)

이것은 58퍼센트의 확률로 bream이나옴. 왜냐면 bream데이타수/총 데이터수 = 0.58이기때문에. 위에꺼랑다른것은, ()일경우에는 바라보는데이터가 정의되지않아서..? 1:1로 대응하면 전부 자기랑 정답이니까.. 근데 2번째거처럼 모든데이타를 바라보면, 1인건 정답이지만 0인건 오답이라..?

5. train/test set

train_input/target과 test_input/target을 분리한다.
연습할때는 train, 실험해볼때는 test로 해본다.

*여기서 A집단은 array = [, , , ]이고 fish_data= A집단+ B집단.

train_input = fish_data[:len(A집단)]
train_target =fish_target[: len(A집단)]

test_input = fish_data[len(A집단):]
test_target = fish_target[len(A집단):]

kn = KNeighborsClassifier()
kn = kn.fit(train_input, train_target)

kn.score(test_input, test_target)

대신 이렇게하면, train할때와 test할때 데이터가 딴판일수 있기때문에, (A집단으로 훈련했는데, 정작 물어보는건 B집단이라, A,B를 골고루 섞어야할 필요가있음)

numpy를 이용해야한다.

import numpy as np

input_arr = np.array(fish_data)
target_arr = np.array(fish_target)

np.array하지않았을때는 [ [] , [] , [] ] 의 모양을 가지지만
np.array를하면 쉼표가 사라지고
[
[]
[]
[]
]의 모습을 가진다
 
index = np.arange(len(A+B집단))     #arange(N)은 0~N까지의 숫자로 이루어진 array생성. 0번째 index엔 0, 1th index엔 1....  
np.random.shuffle(index)  #저 어레이를 랜덤하게 섞음. 

이것을 해준후,
train_input = input_arr[index[: len(A집단)]]        
train_target = target_arr[index[:len(A집단)]]

test_input = input_arr[index[len(A집단): ]]
test_target = target_arr[index[len(A집단): ]]

#원리는, index[: k]로 무작위의 인덱스배열중 0~k까지 불러들여.
그리고 input_arr[ a,b,c]를 해주는데 이것은 [input_arr[a],input_arr[b],input_arr[c]]하는것과같음
#이것들은 배열 슬라이싱인데, input_arr 의 index[0:a집단의수]면 (index에있는 0~집단의 수th 번째 원소에 해당하는 값)번째에 있는 input_arr의 원소들을 반환해

예를 들면
a= np.array([5,6,7,8])
print(a[[1,3]]) 이거하면, 6,8이 불려나와.

이렇게한것을
plt.scatter(train_input[:,0], train_input[:, 1])
plt.scatter(test_input[:,0], test_input[:,1])
plt.xlabel('length')
plt.ylabel('width')
plt.show()
로 테스트해볼수 있는데, 이러면 train과 test가 잘 섞인것을 볼 수 있어.

여기서 train_input[: ,0]이런게 있을텐데  [a:b, c:d]라고하면 a~b까지 row, c~d까지의 col을 선택하는건데 예제에서는 , 앞에는 0~모두라 생략함. 그리고 ,뒤에는 0번째 col만 선택하는건데, 이번예제에서는 [[길이, 넓이], [길이, 넓이]...] 이런식으로 약속이 되있기때문에, 길이만 전체선택한다고 볼 수 있다.

: 는 모두를 의미
, 앞의 수는 row을 의미
, 뒤의 수는 col을 의미

따라서 모든 1번째 col선택은 arr[ :, 1]이 되겠다.

6. numpy와 sklearn으로 세련된 코딩하기
***numpy의 기능을 이용해 간단하게 표현할 수 있다.***
# fish_data =[[l,w] for l, w in zip(length, weight)]
fish_data = np.column_stack((length, weight))
 col1에 길이 데이터, col2에 무게 데이터를 넣는다는뜻.

# fish_target = [1]*len(bream_length) + [0]*len(smelt_length)
fish_target = np.concatenate((np.ones(len(bream_length)), np.zeros(len(smelt_length))))

np.ones/zeros(N)이면 N개의 1로 이루어진 arr를 생성
np.concatenate(arr1, arr2)는 arr1과 arr2를 합침

ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ
from sklearn.model_selection import train_test_split


train_input, test_input, train_target, test_target = train_test_split(
    fish_data, fish_target, stratify= fish_target, random_state=len(fish_data))

이렇게해주면, 
train_input과 test_input 이 나뉘고 train_target과 test_target이 나뉜다.
stratify가 fish_target의 데이터를 보고 그것들이 골고루 섞일 수 있도록 섞는다. 그래서 1,0밖에없는 target이 골고루 섞인상태가되고 마지막 random_state로 어느 한기점을 중심으로 반토막내버려. 그 반토막을 기준으로 train과 test 집단이 나뉘게 되는데, 자연스래 train과 test 값들이 골고루 쪼개질 수 밖에없다.


입력데이터로부터 가까운 이웃까지 거리와 가까운 이웃의 좌표를 반환한다.
이때 가까운 이웃은 kneighbor에 기반한다. kneighbor의 기본값은 5이다.
#returns distance from point to its neighbors and indexes of neighbors based on Kn_neighbors 
distances, indexes = kn.kneighbors([[25,150]])
# print(distances) index is generated based on the graph.
# print(indexes)
[[130.48375378 138.37485321 140.64938677 140.72046759 140.81068851]]
[[ 3 23 16 22 21]] 이런 형식으로.


plt.scatter(train_input[:,0], train_input[:, 1])
# mark triangle on input point 삼각형으로 입력값의 좌표를 표시한다.
plt.scatter(25,150, marker= '^')

# mark diamond on points close to input point 다이아몬드로 가까운이웃을 표시한다.
plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')

이렇게했을때, 육안으로 봣을때 가까운 이웃들이 선택되지않았다면 그것은 아마 그래프 스케일의 문제일 것이다. 예를들어 x: 0~10 이고 y:0~1000이라면, 육안으로 비율이 같게 정사각형모양의 그래프에서는 5,500은 6,700 이 더 가까워보일 수 있지만 실제로는 1,400이 더가깝다. 스케일 조정으로 이런 치사한 오류를 피할 수 있다. 

plt.xlim( min, max) 로 y축에맞게 x축을 조절해보자. 그럼 왜 이런일이 있는지 알 수 있다.

표준점수로 바꿔 이 문제를 해결해보자.
mean = np.mean(train_input, axis=0)
std = np.std(train_input, axis=0)

train_scaled = (train_input - mean)/std

mean의 크기는 train_input과 같기때문에 바로위의 코드를 실행하면 모든 train_iput에서 mean의 값을 빼주고 std로 나눠준다.

test_input의 값들 또한 평균/표준편차를 이용해 scale을 맞춰준다.


kn = kn.fit(train_scaled, train_target)
test_scaled = (test_input - mean)/std
kn.score(test_scaled, test_target)

입력값또한 scale을 맞춰준다.

new = ([25,150]- mean)/std
distances, indexes = kn.kneighbors([new])

그리고 맞춰진 애들을 가지고 새로운 그래프를 그려보자

plt.scatter(train_scaled[:,0], train_scaled[:, 1])
# mark triangle on input point
plt.scatter(new[0], new[1], marker = '^')
# mark diamond on points close to input point 
plt.scatter(train_scaled[indexes,0], train_scaled[indexes,1], marker='D')

plt.xlabel('length')
plt.ylabel('weight')
plt.show()

이렇게하면 육안으로도 공감/이해할 수있는 이웃들이 다이아몬드로 마크된다. 
이렇게 스케일을 맞춰주는 작업을 전처리 작업이라고한다. tree에서는 할 필요가 없다고한다.

7. 회귀(regression)에는 target값을 맞출 필요가없다.예측값이기때문에 이미있는 값(위에서 target [1,0])이아니라 새로운값을 예측하기때문에. 
왜 회귀라고 부르냐면 콜턴이라는 애가 19세기에 키 큰 부모의 자식이 그정도로 키가 크기않고 동년배 평균값의 키로 회귀하드라 라고 예측을 한 논문에서 regression이라는 단어가 굳어졌다...

위의 방식 (최근접분류)과 회귀가 다른것은 최근접분류는 가까운 이웃이 ㅁ,ㅁ,ㅅ이면 타겟의 값은 ㅁ으로 분류하는거고
최근접회귀는  만약 가까운이웃이 10 20 30 이라는 값을 가지고 있다면 [10+20+30]/ 3 = 20 의 값으로 예측하는거다. 

8. 회귀문제에선 stratify 가 필요없다.
#we don't need stratify for regression.
train_input, test_input, train_target, test_target = train_test_split(
    perch_length, perch_weight, random_state= 20
)
이러면 perch_length를 train과 test input이 나눠먹고, perch_weight를 train과 test target이 나눠먹어. 
classification에선 
[
 [], 
 []
] 이렇게나뉘었다.

여기서 왜 모양이 1자로 나오나면, perch_length가 그냥 [ , , ,]형식의 데이터였기때문이야. 저 위에서 classifier할때는 fish_data = np.column_stack(length, width)해서 stack모양으로 만들어줬음.

이러면 모양이 [ [v1,v2] , [v3,v4], .....]이런모양이나오기때문에, 
[
[v1,v2],
[v3,v4]
...
]
이렇게 하고싶다면 reshape를 써라. -> knr.fit에 필요함. 

train_input = np.array(train_input).reshape(-1,1)
test_input = np.array(test_input).reshape(-1,1)

reshape( 줄 갯수,1 set당 원소의 갯수)
줄 갯수가 -1일경우, 알아서 해라라는뜻? 

from sklearn.neighbors import KNeighborsRegressor

knr = KNeighborsRegressor()
knr.fit(train_input, train_target)

knr.score(test_input, test_target)
이걸로 정확도 테스트를 할 수 있는데, 그 공식은 R^2 = 1 - (sum of (target-input)^2 )/(sum of(target-mean)^2) 라고한다. 식을보면, 분수중 위의 target-input값이 0이면, 뺄셈의 값이 1이기때문에 매우 정확하게 된다는걸 알수 있고, 분자와 분모의 값이 같아질수록 뺄셈의 값이 0이기때문에 정확도가 떨어진다는것을 알수 있다. 다시말하면 예측값이 평균과 같아질수록 정확도는 떨어진다..?ㄷㄷ

9. 오차
아래코드는 오차값을 나타내준다.
**예제들에서 input = 길이 target=무게 였던것을 상기하자.

from sklearn.metrics import mean_absolute_error
test_prediction = knr.predict(test_input)
mae = mean_absolute_error(test_target, test_prediction)

결과)
[[33.5]  print(test_input)
 [32. ]
 [31. ]
 [29. ]
 [26.3]]
[675.0, 615.0, 600.0, 450.0, 450.0]  print(test_target)
[649.   602.   550.   493.   471.06] print(test_prediction)
30.612000000000013 print(mae)


코드를보면 test_prediction은 test_input (length)값에 기초해서 weight를 예측한다. 그것이 실제 target (weight)값과 비교했을때 오차가 좀 있는데, 그것이 mae이다. 

10. 과대적합/ 과소적합
print("train score:",knr.score(train_input, train_target) , "\ntest score",knr.score(test_input, test_target))
과소적합 (underfitting): train score < test score. 연습할때보다 실제시험에서 더 잘맞춤
과대적합 (overfitting): train score > test score.  연습할땐 잘했는데 실제시험 조짐

해소법은 knr.n_neighbors를 조절하면된다. 기본값이 5인데, 그것보다 높으면 과소적합, 낮으면 과대적합 
k가 높아질수록 train때보다 test값이 커진다. 
k가 낮아질수록 train때보다 test값이 작아진다.

11. 코렙에서 새로운 노트북을 해야 새로운 파일이 드라이브에 저장된다. 그렇지 않으면 계속하나의 파일에 덮어쓰기하는거임..

12. MachineLearning 5
최근접이웃에 문제점은 샘플범위밖의 수치가 주어졌을때, 그 값에 대한 결과를 주는게아니라, 걔랑 근접한 이웃을 참고해서 값을 반환하기때문에, y = x라는 그래프를 가지고 있어도 샘플이 [0~10]만 주어지고 실제 인풋은 100이라면 100을 반환하는게아니라 10을 반환함. 10이가장 가까운친구이기 때문에.

선형회귀 (linear regression)을 써야한다.
from sklearn.linear_model import LinearRegression

lr = LinearRegression()

lr.fit(train_input, train_target)

coef = np.array(lr.coef_).reshape(-1,)
intercept = np.array(lr.intercept_).reshape(-1,)
print("y = ", coef,"x + ", intercept) #coef와 intercept는 [[a]]의 형태를 가지고있다.

predicted_weight = lr.predict([[input_length]])
print("linear regression:", predicted_weight)
plt.scatter(train_input, train_target)
plt.scatter( input_length, predicted_weight, marker ='^')
plt.show()

이것에 대한 결과는 이미지파일로 저장해두었다. 아주 강력한기능이다. knr은 그 주변 이웃들의 결과의 평균을 산출했다면, linear은 데이터들을 그래프로 그리고 그것에 대한 기울기와 절편을 구해서 x에 input값을 대입해서 y를 구하는 느낌이다.

ymin = 20*coef + intercept
ymax = 40*coef + intercept
print(ymin, ymax)
plt.plot([20, 40], [ymin, ymax])
그래프를 그릴 수 있다. 꼭 reshape(-1, )를 해줘야. 줄갯수가 0이되기때문에 2차원배열이아니라 1차원배열로 된다. 그래야 plot을 그릴 수 있어. 20은 데이터의 가장작은 값보다 작은값 40은 가장큰값보다 큰값으로 설정해서 그래프의 나아가는 방향을 나타내고자했다.

단점이라면 선형그래프이기때문에 음수가 나올 수 있다는점.그리고 곡선은 표현하지않는다는점..

13. 그래서 배우는 다항회귀 (2차,3차.,.ㅜㅜ)

2차원그래프그리는 식이,, y = ax^2 + bx + c 라고한다.. 파이썬에서 제곱은 **2로표현한다.
train_poly = np.column_stack((train_input**2, train_input))
test_poly = np.column_stack((test_input**2, test_input)


lr = LinearRegression()
lr.fit(train_poly, train_target)

predicted_weight_poly = lr.predict([[input_length**2, input_length]]) #입력값을 사용한 예측값

coef_poly = np.array(lr.coef_).reshape(-1,)       #반드시 이렇게 해야 plot으로 그릴 수 있음. 1차원化
intercept_poly = np.array(lr.intercept_).reshape(-1,)

plt.scatter(train_input, train_target) #학습데이터 그래프에 표시하기
plt.scatter(input_length, predicted_weight_poly, marker ='^') #예상지점 ^으로 표시하기

print(coef_poly ,intercept_poly) #[2.90924793 -142.59537447] [2179.20603384]
point = np.arange(20,40) #20은 데이터상 가장작은값보다 작은값 40은 가장큰값보다 큰값. plot에서 선형일때는 [20,40]안에서 내맘대로 쭉긋고쓰면 됐지만 선형이라 모든 포인트가 필요해서이렇게 쓰는듯하다. 선형은 시작지점, 끝지점을 정했잖아.
plt.plot(point, coef_poly[0]*point**2 +coef_poly[1]*point + intercept_poly) # y = ax^2+bx+c where a =coef[0], b= coef[1], c= intercept.

#print(lr.score(train_poly, train_target))
#print(lr.score(test_poly, test_target))
plt.show()


14. MachineLearning 6.
3개의 특성(Attributes)이 주어졌을때 예측..
import pandas as pd

df = pd.read_csv('https://bit.ly/perch_csv')

print(df) #0      8.4     2.11    1.41 이런형태야. index length height width
perch_full = df.to_numpy()

print(perch_full) [ [ 8.4   2.11  1.41], [] ...] 이런형태로 바꿔줘. 근데 이거 stack이 되어있음.

이런 코드로 남이 업로드해논 데이터를 가져올 수 있다. 그리고 to_numpy()를 이용해 2차원배열로 바꿀 수 있어.
판다스는 엑셀맹키로 생긴 파일을 읽어올 수 있다네. 시각화기능도있다고함.

PolynomialFeature를 Transformer라 부르고 LinearRegression, KNeighbors를 Estimuator라고부른다.
Transformer: fit -> transform
Estimator: fit -> predict-> score

from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures()
poly.fit([[2,3]])
print(poly.transform([[2,3]])) #[[1. 2. 3. 4. 6. 9.]]

poly.fit([[a,b]]) -> poly.transform([[a,b]]) 의 결과는 [[1, a, b, a^2, a*b ,b^2]]

train_input, test_input, train_target, test_target = train_test_split(
    perch_full, perch_weight, random_state= len(perch_full)
)
train_input = np.array(train_input).reshape(-1,3) #주의할점은 -1,3이라는것. 3은 한 줄에 3개의 특성이 다 담겨있다는것이야. 이래야지만 train_poly.shape가 42, 9 이렇게나옴.
test_input = np.array(test_input).reshape(-1,3)
#do this, otherwise we have typeerror: only integer scalar arrays can be converted to a scalar index site:stackoverflow.com
train_target = np.array(train_target).reshape(-1,1) #여기는 -1,1로해주는게중요하다. 왜냐면 얘네들은 무게의 데이터를 가지고 있기때문에 한줄에 1개의 값만들어간다. 

#귀찮아서 false로 해준다함.. 1을 굳이할 필요없어서?
poly = PolynomialFeatures(include_bias= False) #기본degree는 2이다.
poly.fit(train_input) #(42, 9) 42개의 행, 9개의 열(특성) 밑에거.
train_poly = poly.transform(train_input) #['x0' 'x1' 'x2' 'x0^2' 'x0 x1' 'x0 x2' 'x1^2' 'x1 x2' 'x2^2'] 모든 특성이 다들어가 융합된모습이야. a,b,c,a^2, ab,ac,b^2,bc,c^2 의 모습
>

test_poly = poly.transform(test_input) #훈련세트를 변환한거맹키로 테스트세트도 변환해주는 습관을 들여라.

poly = PolynomialFeatures(degree=5, include_bias= False) 이렇게 degree를바꾸면
print(train_poly.shape) #(42,55)가되고 
print(lr.score(train_poly, train_target)) 하면 0.9999999975216065 라서 잘나오는데..
print(lr.score(test_poly, test_target)) 하면 -44816.57139864114 곱창난다. 

너무이렇게 과대적합으로 곱창나면 규제(정규화)를 해줘야한다. 가중치(기울기)를 작게한다. 

from sklearn.preprocessing import StandardScaler
ss = StandardScaler();
ss.fit(train_poly) #여기서 표준편차를 구한다음

train_scaled = ss.transform(train_poly) #여기서 알아서 스케일 맞춰준다. 아마(x-mean)/std겟지
test_scaled = ss.transform(test_poly)

KNeightbor할때 스케일을 맞췄던것처럼 Regression에도 그래줘야한다. 스케일을 맞추고나서
그리고 LinearRegression에 평가를 요구하기보다 Ridge에 평가를 받아본다. 릿지는 가중치의 제곱을 기준으로 계산한다.
from sklearn.linear_model import Ridge
ridge = Ridge() #default alpha =1. 높아질수록 감도가올라가는데 베스트감도는 일일히 맞춰갈 수 밖에없음
ridge.fit(train_scaled, train_target)

print(ridge.score(train_scaled, train_target))
print(ridge.score(test_scaled, test_target)) #둘다 95퍼센트의 정확도를 가짐


plt.scatter(train_poly[:,0], train_target)
plt.scatter(train_poly[:,1], train_target)
plt.scatter(train_poly[:,2], train_target)
plt.show()

plt.scatter(train_scaled[:,0], train_target)
plt.scatter(train_scaled[:,1], train_target)
plt.scatter(train_scaled[:,2], train_target)
plt.show()

스케일을 맞춘것과 안맞춘것의 그래프 차이는 이미지로 저장되었다. scale을 맞추지않은것은 자기값에 따라 나아가는 방향과 기울기가 다르지만, 스케일을 맞추고나서는 단일화하여 하나의 비슷한 방향으로 가는것으로 관측된다.

베스트 값을 찾는 법은, 

alpha_list = [0.00000001,0.0000001,0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000]
train_score_list = []
test_score_list = []
for alpha in alpha_list:
  ridge = Ridge(alpha=alpha)
  
  ridge.fit(train_scaled, train_target)
  train_score_list.append(ridge.score(train_scaled, train_target))
  test_score_list.append(ridge.score(test_scaled, test_target))

plt.plot(np.log10(alpha_list), train_score_list)
plt.plot(np.log10(alpha_list), test_score_list)
plt.show()
이렇게 그래프를 그리고, 가장 서로의 그래프의 간격이 적은 쪽의 alpha값을 취한다. 왼쪽으로 갈수록 과대적합이고 오른쪽으로 갈수록 과소적합이된다. 왼쪽으로 갈 수록 훈련세트 위주로 훈련을 하는거니까 실제 값은 훈련세트랑 다르니까 빗나가는거임. 반대로 오른쪽으로 갈 수록 훈련세트를 무시하니까 훈련세트의 정확도가 감소.

Lasso 회귀라는것도 있는데, 이것은 가중치의 절대값으로 계산한다.

from sklearn.linear_model import Lasso
#Lasso Regression
lasso = Lasso(alpha=1)
lasso.fit(train_scaled, train_target)

print(lasso.score(train_scaled, train_target))
print(lasso.score(test_scaled, test_target))
print("# of attributes using Lasso: ",np.sum(lasso.coef_==0)) 이것을 통해 몇개의 특성이 Lasso에 영향을 받는지 알 수 있다. 예를들어 특성 55개중 이 값이 40일경우, 15(55-40)개만 Lasso회귀를 사용하고 나머지는 사용하지않는다는것을 의미한다. 

베스트 알파값을 찾는것의 방법은 릿지때와같다.

#find Best alpha value 
alpha_list = [0.001,0.01,0.1,1,10,100,1000]
train_score_list = []
test_score_list = []
for alpha in alpha_list:
  lasso = Lasso(alpha= alpha)
  
  lasso.fit(train_scaled, train_target)
  train_score_list.append(lasso.score(train_scaled, train_target))
  test_score_list.append(lasso.score(test_scaled, test_target))

plt.plot(np.log10(alpha_list), train_score_list)
plt.plot(np.log10(alpha_list), test_score_list)
plt.show()

이로써 LinearRegression, Ridge, Lasso를 알아보았다. 각각 score를 메길 수 잇는것을 확인했다. 아마도 하는 이유는 훈련데이터를 자기나름대로 분석해서 신뢰를 얻기위함인것같다... 미래예측할때 자기가 가장 신뢰하는 방법을 사용하는 거아닐까?  그런데, 특성(Attributes)이 2개이상인것을 쓸때는 LinearRegression을 쓰지마?????. 아마 과대적합문제일텐데, Ridge와 Lasso는 그 문제가 적다.대신에 예측값은 대충 비슷하게나온다.



15.
