https://colab.research.google.com/?hl=ko

접속 취소하고 
연결 ->  호스팅된 런타임연결

컨트롤 엔터로 실행
알트 엔터로 실행 후 아래줄이동
시프트 엔터로 아랫줄로이동

구글드라이브나 github에 저장가능'

사용하게될 라이브러리는 sklearn , tensorflow다.

런타임은 평소에는 none이지만 tensorflow쓸땐 GPU로 설정.

1. 그래프만들기
import matplotlib.pyplot as plt

bream_length = [25.4,26.3,26.5,29.0,29.0,29.7,29.7,30.0,30.7,30.9,31.0,31.0,31.5,32.0,32.0,33.0,33.5,33.5,34.0,34.0]
# print(len(bream_length))
bream_weight= [430.3,450.0,500.0,390.0,450.0,500.0,475.0,500.0,500.0,340.0,600.0,600.0,700.0,700.0,610.0,650.0,575.0,685.0,620.0,680.0]

plt.scatter(bream_length, bream_weight)
plt.xlabel('length')
plt.ylabel('weight')

plt.show()

신기하다. 원리는 plt.scatter(X축, Y축) 과 show

하나의 그래프에 여러개의 데이터를 넣을 수 있다. 그럼 알아서 색깔도 다르게해줌
plt.scatter( data1, data1_)
plt.scatter(data2, data2_) 

2. 데이터합치기 for 분류 (Classifier)

length = bream_length + smelt_length
weight = bream_weight+ smelt_weight

fish_data =[[l,w] for l, w in zip(length, weight)]

이렇게하면 [  [l1, w1].... [ln, wn]]  2 dimensional array가 만들어져.

이렇게합쳐놓고, 뭐가 뭔지 지정을 해줘야해. (정답지)

fish_target = [1]*len(bream_length) + [0]*len(smelt_length) 이렇게하면 bream은 1로써 저장되고 smelt는 0으로써 저장되겠지. 찾을려는 정보를 1로 놓는게 좋다. 
[1,1,1,1,1,1,1,1,0,0,0,0,0,0,0] 이런모양이 돼.

***numpy의 기능을 이용해 간단하게 표현할 수 있다.***
# fish_data =[[l,w] for l, w in zip(length, weight)]
fish_data = np.column_stack((length, weight))
 col1에 길이 데이터, col2에 무게 데이터를 넣는다는뜻.
[ [ l1, w1 ], 
   [l2, w2]...] 이렇게 저모양으로 스텍이쌓이네요


# fish_target = [1]*len(bream_length) + [0]*len(smelt_length)
fish_target = np.concatenate((np.ones(len(bream_length)), np.zeros(len(smelt_length))))

np.ones/zeros(N)이면 N개의 1로 이루어진 arr를 생성
np.concatenate(arr1, arr2)는 arr1과 arr2를 합침

3. K-최근접이웃
from sklearn.neighbors import KNeighborsClassifier

kn = KNeighborsClassifier()

//machine learning model
kn.fit(fish_data, fish_target)   
반반나눈 data와 target을 묶는다.. data값일때 정답은 target이게.. 

//그냥 얼마나 잘맞추는지. 1= 100%정답률. = 여기선 당연히 그래야하는게 섞을때 같은 index로했기때문에.. 예를들어 data[n]가 '대한민국'였을경우 target[n]='Korea'이기때문에 항상일치.
kn.score(fish_data, fish_target)

 새로운 데이터를 주고 어느 집단에속하는지 알게하는법

kn.predict([[2, 10]])
이렇게주면 0이나오고 [[30,600  ]] 이런걸로주면 1로나옴.! 가장 가까운 이웃들이 어떤 데이터인지 보고 주어진 데이터의 값도 이웃들과 비슷하겠거니함.. 


4. 정확도
kn.predict([[30, 500]])

kn49 = KNeighborsClassifier(n_neighbors = (len(bream_length)+len(smelt_length)))
kn49.fit(fish_data, fish_target)
kn49.score(fish_data, fish_target)

이것은 58퍼센트의 확률로 bream이나옴. 왜냐면 bream데이타수/총 데이터수 = 0.58이기때문에. 위에꺼랑다른것은, ()일경우에는 바라보는데이터가 정의되지않아서..? 1:1로 대응하면 전부 자기랑 정답이니까.. 근데 2번째거처럼 모든데이타를 바라보면, 1인건 정답이지만 0인건 오답이라..?

5. train/test set

train_input/target과 test_input/target을 분리한다.
연습할때는 train, 실험해볼때는 test로 해본다.

*여기서 A집단은 array = [, , , ]이고 fish_data= A집단+ B집단.

train_input = fish_data[:len(A집단)]
train_target =fish_target[: len(A집단)]

test_input = fish_data[len(A집단):]
test_target = fish_target[len(A집단):]

kn = KNeighborsClassifier()
kn = kn.fit(train_input, train_target)

kn.score(test_input, test_target)

대신 이렇게하면, train할때와 test할때 데이터가 딴판일수 있기때문에, (A집단으로 훈련했는데, 정작 물어보는건 B집단이라, A,B를 골고루 섞어야할 필요가있음)

numpy를 이용해야한다.

import numpy as np

input_arr = np.array(fish_data)
target_arr = np.array(fish_target)

np.array하지않았을때는 [ [] , [] , [] ] 의 모양을 가지지만
np.array를하면 쉼표가 사라지고
[
[]
[]
[]
]의 모습을 가진다
 
index = np.arange(len(A+B집단))     #arange(N)은 0~N까지의 숫자로 이루어진 array생성. 0번째 index엔 0, 1th index엔 1....  
np.random.shuffle(index)  #저 어레이를 랜덤하게 섞음. 

이것을 해준후,
train_input = input_arr[index[: len(A집단)]]        
train_target = target_arr[index[:len(A집단)]]

test_input = input_arr[index[len(A집단): ]]
test_target = target_arr[index[len(A집단): ]]

#원리는, index[: k]로 무작위의 인덱스배열중 0~k까지 불러들여.
그리고 input_arr[ a,b,c]를 해주는데 이것은 [input_arr[a],input_arr[b],input_arr[c]]하는것과같음
#이것들은 배열 슬라이싱인데, input_arr 의 index[0:a집단의수]면 (index에있는 0~집단의 수th 번째 원소에 해당하는 값)번째에 있는 input_arr의 원소들을 반환해

예를 들면
a= np.array([5,6,7,8])
print(a[[1,3]]) 이거하면, 6,8이 불려나와.

이렇게한것을
plt.scatter(train_input[:,0], train_input[:, 1])
plt.scatter(test_input[:,0], test_input[:,1])
plt.xlabel('length')
plt.ylabel('width')
plt.show()
로 테스트해볼수 있는데, 이러면 train과 test가 잘 섞인것을 볼 수 있어.

여기서 train_input[: ,0]이런게 있을텐데  [a:b, c:d]라고하면 a~b까지 row, c~d까지의 col을 선택하는건데 예제에서는 , 앞에는 0~모두라 생략함. 그리고 ,뒤에는 0번째 col만 선택하는건데, 이번예제에서는 [[길이, 넓이], [길이, 넓이]...] 이런식으로 약속이 되있기때문에, 길이만 전체선택한다고 볼 수 있다.

: 는 모두를 의미
, 앞의 수는 row을 의미
, 뒤의 수는 col을 의미

따라서 모든 1번째 col선택은 arr[ :, 1]이 되겠다.

6. numpy와 sklearn으로 세련된 코딩하기
***numpy의 기능을 이용해 간단하게 표현할 수 있다.***
# fish_data =[[l,w] for l, w in zip(length, weight)]
fish_data = np.column_stack((length, weight))
 col1에 길이 데이터, col2에 무게 데이터를 넣는다는뜻.

# fish_target = [1]*len(bream_length) + [0]*len(smelt_length)
fish_target = np.concatenate((np.ones(len(bream_length)), np.zeros(len(smelt_length))))

np.ones/zeros(N)이면 N개의 1로 이루어진 arr를 생성
np.concatenate(arr1, arr2)는 arr1과 arr2를 합침

ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ
from sklearn.model_selection import train_test_split


train_input, test_input, train_target, test_target = train_test_split(
    fish_data, fish_target, stratify= fish_target, random_state=len(fish_data))

이렇게해주면, 
train_input과 test_input 이 나뉘고 train_target과 test_target이 나뉜다.
stratify가 fish_target의 데이터를 보고 그것들이 골고루 섞일 수 있도록 섞는다. 그래서 1,0밖에없는 target이 골고루 섞인상태가되고 마지막 random_state로 어느 한기점을 중심으로 반토막내버려. 그 반토막을 기준으로 train과 test 집단이 나뉘게 되는데, 자연스래 train과 test 값들이 골고루 쪼개질 수 밖에없다.


입력데이터로부터 가까운 이웃까지 거리와 가까운 이웃의 좌표를 반환한다.
이때 가까운 이웃은 kneighbor에 기반한다. kneighbor의 기본값은 5이다.
#returns distance from point to its neighbors and indexes of neighbors based on Kn_neighbors 
distances, indexes = kn.kneighbors([[25,150]])
# print(distances) index is generated based on the graph.
# print(indexes)
[[130.48375378 138.37485321 140.64938677 140.72046759 140.81068851]]
[[ 3 23 16 22 21]] 이런 형식으로.


plt.scatter(train_input[:,0], train_input[:, 1])
# mark triangle on input point 삼각형으로 입력값의 좌표를 표시한다.
plt.scatter(25,150, marker= '^')

# mark diamond on points close to input point 다이아몬드로 가까운이웃을 표시한다.
plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')

이렇게했을때, 육안으로 봣을때 가까운 이웃들이 선택되지않았다면 그것은 아마 그래프 스케일의 문제일 것이다. 예를들어 x: 0~10 이고 y:0~1000이라면, 육안으로 비율이 같게 정사각형모양의 그래프에서는 5,500은 6,700 이 더 가까워보일 수 있지만 실제로는 1,400이 더가깝다. 스케일 조정으로 이런 치사한 오류를 피할 수 있다. 

plt.xlim( min, max) 로 y축에맞게 x축을 조절해보자. 그럼 왜 이런일이 있는지 알 수 있다.

표준점수로 바꿔 이 문제를 해결해보자.
mean = np.mean(train_input, axis=0)
std = np.std(train_input, axis=0)

train_scaled = (train_input - mean)/std

mean의 크기는 train_input과 같기때문에 바로위의 코드를 실행하면 모든 train_iput에서 mean의 값을 빼주고 std로 나눠준다.

test_input의 값들 또한 평균/표준편차를 이용해 scale을 맞춰준다.


kn = kn.fit(train_scaled, train_target)
test_scaled = (test_input - mean)/std
kn.score(test_scaled, test_target)

입력값또한 scale을 맞춰준다.

new = ([25,150]- mean)/std
distances, indexes = kn.kneighbors([new])

그리고 맞춰진 애들을 가지고 새로운 그래프를 그려보자

plt.scatter(train_scaled[:,0], train_scaled[:, 1])
# mark triangle on input point
plt.scatter(new[0], new[1], marker = '^')
# mark diamond on points close to input point 
plt.scatter(train_scaled[indexes,0], train_scaled[indexes,1], marker='D')

plt.xlabel('length')
plt.ylabel('weight')
plt.show()

이렇게하면 육안으로도 공감/이해할 수있는 이웃들이 다이아몬드로 마크된다. 
이렇게 스케일을 맞춰주는 작업을 전처리 작업이라고한다. tree에서는 할 필요가 없다고한다.

7. 회귀(regression)에는 target값을 맞출 필요가없다.예측값이기때문에 이미있는 값(위에서 target [1,0])이아니라 새로운값을 예측하기때문에. 
왜 회귀라고 부르냐면 콜턴이라는 애가 19세기에 키 큰 부모의 자식이 그정도로 키가 크기않고 동년배 평균값의 키로 회귀하드라 라고 예측을 한 논문에서 regression이라는 단어가 굳어졌다...

위의 방식 (최근접분류)과 회귀가 다른것은 최근접분류는 가까운 이웃이 ㅁ,ㅁ,ㅅ이면 타겟의 값은 ㅁ으로 분류하는거고
최근접회귀는  만약 가까운이웃이 10 20 30 이라는 값을 가지고 있다면 [10+20+30]/ 3 = 20 의 값으로 예측하는거다. 

8. 회귀문제에선 stratify 가 필요없다.
#we don't need stratify for regression.
train_input, test_input, train_target, test_target = train_test_split(
    perch_length, perch_weight, random_state= 20
)
이러면 perch_length를 train과 test input이 나눠먹고, perch_weight를 train과 test target이 나눠먹어. 
classification에선 
[
 [], 
 []
] 이렇게나뉘었다.

여기서 왜 모양이 1자로 나오나면, perch_length가 그냥 [ , , ,]형식의 데이터였기때문이야. 저 위에서 classifier할때는 fish_data = np.column_stack(length, width)해서 stack모양으로 만들어줬음.

이러면 모양이 [ [v1,v2] , [v3,v4], .....]이런모양이나오기때문에, 
[
[v1,v2],
[v3,v4]
...
]
이렇게 하고싶다면 reshape를 써라. -> knr.fit에 필요함. 

train_input = np.array(train_input).reshape(-1,1)
test_input = np.array(test_input).reshape(-1,1)

reshape( 줄 갯수,1 set당 원소의 갯수)
줄 갯수가 -1일경우, 알아서 해라라는뜻? 

from sklearn.neighbors import KNeighborsRegressor

knr = KNeighborsRegressor()
knr.fit(train_input, train_target)

knr.score(test_input, test_target)
이걸로 정확도 테스트를 할 수 있는데, 그 공식은 R^2 = 1 - (sum of (target-input)^2 )/(sum of(target-mean)^2) 라고한다. 식을보면, 분수중 위의 target-input값이 0이면, 뺄셈의 값이 1이기때문에 매우 정확하게 된다는걸 알수 있고, 분자와 분모의 값이 같아질수록 뺄셈의 값이 0이기때문에 정확도가 떨어진다는것을 알수 있다. 다시말하면 예측값이 평균과 같아질수록 정확도는 떨어진다..?ㄷㄷ

9. 오차
아래코드는 오차값을 나타내준다.
**예제들에서 input = 길이 target=무게 였던것을 상기하자.

from sklearn.metrics import mean_absolute_error
test_prediction = knr.predict(test_input)
mae = mean_absolute_error(test_target, test_prediction)

결과)
[[33.5]  print(test_input)
 [32. ]
 [31. ]
 [29. ]
 [26.3]]
[675.0, 615.0, 600.0, 450.0, 450.0]  print(test_target)
[649.   602.   550.   493.   471.06] print(test_prediction)
30.612000000000013 print(mae)


코드를보면 test_prediction은 test_input (length)값에 기초해서 weight를 예측한다. 그것이 실제 target (weight)값과 비교했을때 오차가 좀 있는데, 그것이 mae이다. 

10. 과대적합/ 과소적합
print("train score:",knr.score(train_input, train_target) , "\ntest score",knr.score(test_input, test_target))
과소적합 (underfitting): train score < test score. 연습할때보다 실제시험에서 더 잘맞춤
과대적합 (overfitting): train score > test score.  연습할땐 잘했는데 실제시험 조짐

해소법은 knr.n_neighbors를 조절하면된다. 기본값이 5인데, 그것보다 높으면 과소적합, 낮으면 과대적합 
k가 높아질수록 train때보다 test값이 커진다. 
k가 낮아질수록 train때보다 test값이 작아진다.

11. 코렙에서 새로운 노트북을 해야 새로운 파일이 드라이브에 저장된다. 그렇지 않으면 계속하나의 파일에 덮어쓰기하는거임..

12. MachineLearning 5
최근접이웃에 문제점은 샘플범위밖의 수치가 주어졌을때, 그 값에 대한 결과를 주는게아니라, 걔랑 근접한 이웃을 참고해서 값을 반환하기때문에, y = x라는 그래프를 가지고 있어도 샘플이 [0~10]만 주어지고 실제 인풋은 100이라면 100을 반환하는게아니라 10을 반환함. 10이가장 가까운친구이기 때문에.

선형회귀 (linear regression)을 써야한다.
from sklearn.linear_model import LinearRegression

lr = LinearRegression()

lr.fit(train_input, train_target)

coef = np.array(lr.coef_).reshape(-1,)
intercept = np.array(lr.intercept_).reshape(-1,)
print("y = ", coef,"x + ", intercept) #coef와 intercept는 [[a]]의 형태를 가지고있다.

predicted_weight = lr.predict([[input_length]])
print("linear regression:", predicted_weight)
plt.scatter(train_input, train_target)
plt.scatter( input_length, predicted_weight, marker ='^')
plt.show()

이것에 대한 결과는 이미지파일로 저장해두었다. 아주 강력한기능이다. knr은 그 주변 이웃들의 결과의 평균을 산출했다면, linear은 데이터들을 그래프로 그리고 그것에 대한 기울기와 절편을 구해서 x에 input값을 대입해서 y를 구하는 느낌이다.

ymin = 20*coef + intercept
ymax = 40*coef + intercept
print(ymin, ymax)
plt.plot([20, 40], [ymin, ymax])
그래프를 그릴 수 있다. 꼭 reshape(-1, )를 해줘야. 줄갯수가 0이되기때문에 2차원배열이아니라 1차원배열로 된다. 그래야 plot을 그릴 수 있어. 20은 데이터의 가장작은 값보다 작은값 40은 가장큰값보다 큰값으로 설정해서 그래프의 나아가는 방향을 나타내고자했다.

단점이라면 선형그래프이기때문에 음수가 나올 수 있다는점.그리고 곡선은 표현하지않는다는점..

13. 그래서 배우는 다항회귀 (2차,3차.,.ㅜㅜ)

2차원그래프그리는 식이,, y = ax^2 + bx + c 라고한다.. 파이썬에서 제곱은 **2로표현한다.
train_poly = np.column_stack((train_input**2, train_input))
test_poly = np.column_stack((test_input**2, test_input)


lr = LinearRegression()
lr.fit(train_poly, train_target)

predicted_weight_poly = lr.predict([[input_length**2, input_length]]) #입력값을 사용한 예측값

coef_poly = np.array(lr.coef_).reshape(-1,)       #반드시 이렇게 해야 plot으로 그릴 수 있음. 1차원化
intercept_poly = np.array(lr.intercept_).reshape(-1,)

plt.scatter(train_input, train_target) #학습데이터 그래프에 표시하기
plt.scatter(input_length, predicted_weight_poly, marker ='^') #예상지점 ^으로 표시하기

print(coef_poly ,intercept_poly) #[2.90924793 -142.59537447] [2179.20603384]
point = np.arange(20,40) #20은 데이터상 가장작은값보다 작은값 40은 가장큰값보다 큰값. plot에서 선형일때는 [20,40]안에서 내맘대로 쭉긋고쓰면 됐지만 선형이라 모든 포인트가 필요해서이렇게 쓰는듯하다. 선형은 시작지점, 끝지점을 정했잖아.
plt.plot(point, coef_poly[0]*point**2 +coef_poly[1]*point + intercept_poly) # y = ax^2+bx+c where a =coef[0], b= coef[1], c= intercept.

#print(lr.score(train_poly, train_target))
#print(lr.score(test_poly, test_target))
plt.show()


14. MachineLearning 6.
3개의 특성(Attributes)이 주어졌을때 예측..
import pandas as pd

df = pd.read_csv('https://bit.ly/perch_csv')

print(df) #0      8.4     2.11    1.41 이런형태야. index length height width
perch_full = df.to_numpy()

print(perch_full) [ [ 8.4   2.11  1.41], [] ...] 이런형태로 바꿔줘. 근데 이거 stack이 되어있음.

이런 코드로 남이 업로드해논 데이터를 가져올 수 있다. 그리고 to_numpy()를 이용해 2차원배열로 바꿀 수 있어.
판다스는 엑셀맹키로 생긴 파일을 읽어올 수 있다네. 시각화기능도있다고함.

PolynomialFeature를 Transformer라 부르고 LinearRegression, KNeighbors를 Estimuator라고부른다.
Transformer: fit -> transform
Estimator: fit -> predict-> score

from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures()
poly.fit([[2,3]])
print(poly.transform([[2,3]])) #[[1. 2. 3. 4. 6. 9.]]

poly.fit([[a,b]]) -> poly.transform([[a,b]]) 의 결과는 [[1, a, b, a^2, a*b ,b^2]]

train_input, test_input, train_target, test_target = train_test_split(
    perch_full, perch_weight, random_state= len(perch_full)
)
train_input = np.array(train_input).reshape(-1,3) #주의할점은 -1,3이라는것. 3은 한 줄에 3개의 특성이 다 담겨있다는것이야. 이래야지만 train_poly.shape가 42, 9 이렇게나옴.
test_input = np.array(test_input).reshape(-1,3)
#do this, otherwise we have typeerror: only integer scalar arrays can be converted to a scalar index site:stackoverflow.com
train_target = np.array(train_target).reshape(-1,1) #여기는 -1,1로해주는게중요하다. 왜냐면 얘네들은 무게의 데이터를 가지고 있기때문에 한줄에 1개의 값만들어간다. 

#귀찮아서 false로 해준다함.. 1을 굳이할 필요없어서?
poly = PolynomialFeatures(include_bias= False) #기본degree는 2이다.
poly.fit(train_input) #(42, 9) 42개의 행, 9개의 열(특성) 밑에거.
train_poly = poly.transform(train_input) #['x0' 'x1' 'x2' 'x0^2' 'x0 x1' 'x0 x2' 'x1^2' 'x1 x2' 'x2^2'] 모든 특성이 다들어가 융합된모습이야. a,b,c,a^2, ab,ac,b^2,bc,c^2 의 모습
>

test_poly = poly.transform(test_input) #훈련세트를 변환한거맹키로 테스트세트도 변환해주는 습관을 들여라.

poly = PolynomialFeatures(degree=5, include_bias= False) 이렇게 degree를바꾸면
print(train_poly.shape) #(42,55)가되고 
print(lr.score(train_poly, train_target)) 하면 0.9999999975216065 라서 잘나오는데..
print(lr.score(test_poly, test_target)) 하면 -44816.57139864114 곱창난다. 

너무이렇게 과대적합으로 곱창나면 규제(정규화)를 해줘야한다. 가중치(기울기)를 작게한다. 

from sklearn.preprocessing import StandardScaler
ss = StandardScaler();
ss.fit(train_poly) #여기서 표준편차를 구한다음

train_scaled = ss.transform(train_poly) #여기서 알아서 스케일 맞춰준다. 아마(x-mean)/std겟지
test_scaled = ss.transform(test_poly)

KNeightbor할때 스케일을 맞췄던것처럼 Regression에도 그래줘야한다. 스케일을 맞추고나서
그리고 LinearRegression에 평가를 요구하기보다 Ridge에 평가를 받아본다. 릿지는 가중치의 제곱을 기준으로 계산한다.
from sklearn.linear_model import Ridge
ridge = Ridge() #default alpha =1. 높아질수록 감도가올라가는데 베스트감도는 일일히 맞춰갈 수 밖에없음
ridge.fit(train_scaled, train_target)

print(ridge.score(train_scaled, train_target))
print(ridge.score(test_scaled, test_target)) #둘다 95퍼센트의 정확도를 가짐


plt.scatter(train_poly[:,0], train_target)
plt.scatter(train_poly[:,1], train_target)
plt.scatter(train_poly[:,2], train_target)
plt.show()

plt.scatter(train_scaled[:,0], train_target)
plt.scatter(train_scaled[:,1], train_target)
plt.scatter(train_scaled[:,2], train_target)
plt.show()

스케일을 맞춘것과 안맞춘것의 그래프 차이는 이미지로 저장되었다. scale을 맞추지않은것은 자기값에 따라 나아가는 방향과 기울기가 다르지만, 스케일을 맞추고나서는 단일화하여 하나의 비슷한 방향으로 가는것으로 관측된다.

베스트 값을 찾는 법은, 

alpha_list = [0.00000001,0.0000001,0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000]
train_score_list = []
test_score_list = []
for alpha in alpha_list:
  ridge = Ridge(alpha=alpha)
  
  ridge.fit(train_scaled, train_target)
  train_score_list.append(ridge.score(train_scaled, train_target))
  test_score_list.append(ridge.score(test_scaled, test_target))

plt.plot(np.log10(alpha_list), train_score_list)
plt.plot(np.log10(alpha_list), test_score_list)
plt.show()
이렇게 그래프를 그리고, 가장 서로의 그래프의 간격이 적은 쪽의 alpha값을 취한다. 왼쪽으로 갈수록 과대적합이고 오른쪽으로 갈수록 과소적합이된다. 왼쪽으로 갈 수록 훈련세트 위주로 훈련을 하는거니까 실제 값은 훈련세트랑 다르니까 빗나가는거임. 반대로 오른쪽으로 갈 수록 훈련세트를 무시하니까 훈련세트의 정확도가 감소.
alpha가 규제인데, 규제가 커질수록 과소적합, 적어질수록 과대적합이라고한다.

Lasso 회귀라는것도 있는데, 이것은 가중치의 절대값으로 계산한다.

from sklearn.linear_model import Lasso
#Lasso Regression
lasso = Lasso(alpha=1)
lasso.fit(train_scaled, train_target)

print(lasso.score(train_scaled, train_target))
print(lasso.score(test_scaled, test_target))
print("# of attributes using Lasso: ",np.sum(lasso.coef_==0)) 이것을 통해 몇개의 특성이 Lasso에 영향을 받는지 알 수 있다. 예를들어 특성 55개중 이 값이 40일경우, 15(55-40)개만 Lasso회귀를 사용하고 나머지는 사용하지않는다는것을 의미한다. 

베스트 알파값을 찾는것의 방법은 릿지때와같다.

#find Best alpha value 
alpha_list = [0.001,0.01,0.1,1,10,100,1000]
train_score_list = []
test_score_list = []
for alpha in alpha_list:
  lasso = Lasso(alpha= alpha)
  
  lasso.fit(train_scaled, train_target)
  train_score_list.append(lasso.score(train_scaled, train_target))
  test_score_list.append(lasso.score(test_scaled, test_target))

plt.plot(np.log10(alpha_list), train_score_list)
plt.plot(np.log10(alpha_list), test_score_list)
plt.show()

이로써 LinearRegression, Ridge, Lasso를 알아보았다. 각각 score를 메길 수 잇는것을 확인했다. 아마도 하는 이유는 훈련데이터를 자기나름대로 분석해서 예측값를 얻기위함인것같다... 미래예측할때 자기가 가장 신뢰하는 방법을 사용하는 건가보다.  그런데, 특성(Attributes)이 2개이상인것을 쓸때는 LinearRegression을 쓰지말것이 권장되는듯하다.. 아마 과대적합문제일텐데, Ridge와 Lasso는 그 문제가 적다.대신에 어떤 방법을 쓰든 예측값은 대충 비슷하게나온다.Ridge와 Lasso의 예측값은 그들의 alpha 에 따라 달라지는데, 최선의 알파값은 위의 공식에서 찾을 수 있다. 
*degree가 달라짐에따라 LR도 붕괴하는것을 확인했다. 그냥 Ridge나 Lasso를쓰자.얘네들의 값은 꽤 잘맞춘다.

1줄의 배열은 scale할수없다. 왜냐면 scale의 공식이 (value-mean)/std인데, 여기서 mean이 첫줄의 mean과 같아서 0으로 된 배열을 반환할뿐이다. 따라서 predict할때는 저것을 따라라.
l = 28.4
h = 7.11
w = 4.14
list = np.array([l,h,w]).reshape(-1,3)

poly.fit(list)
input = poly.transform(list)
그리고 poly를 쓴다면, (아마 여러개의 attributes를 쓸때그러겠지), 반드시 실험값도 poly로 전개해줘야한다. 전개를하면 degree에 맞는 형식의 array를 만들수있다. 예를 들어 degree= 2에서는 ['x0' 'x1' 'x2' 'x0^2' 'x0 x1' 'x0 x2' 'x1^2' 'x1 x2' 'x2^2'] 


15. machineLearning7 
럭키백 =  x%확률로 A가 나온다. 메이플 부화기같은거지
분류문제: 왜냐면 인풋값이 주어졌을때 그것의 속성을 맞추는거기때문임. 그것의 값을 계산하는게아니라. 로지스틱회귀(LogisticRegression)이라는 이름을사용하지만 회귀라기보단 분류에 가까움... 공집합모양 = 1/(1+e^(-z)) 을 시그모이드라고 부른다.

판다로 데이터를 url로부터 불러올수있음. 
 fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy() 이런게있는데
이것은 fish 배열의 weight, legnth, diagonal, height, width Column들의 값을 가져오는거임. ㄷ

fish가 [ 
	[species ,weight  ,length  ,diagoanl  ,height  ,width]
   ] 이렇게생겻는데 해당되는 [모든열][가져올것이름] 하면
 그쪽값만 이런모양으로 ->[ 
[],
[].. ] 가져오는듯..

*Split train/test set은 이제 너무나 당연해서 설명조차안한다.
#split train/test set
train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, stratify = fish_target, random_state= len(fish_input)
)

*scale맞추는것도 너무 당연해서 설명안하는듯
#make sacled arrays
mean = np.mean(train_input, axis=0)
std = np.std(train_input, axis=0)
train_scaled = (train_input-mean)/std
test_scaled = (test_input-mean)/std

*그래프로 그림그리기할려면 반드시 reshape해줘야함

ti = np.array(train_input).reshape(-1,5)
tt = np.array(train_target).reshape(-1,)

plt.scatter(ti[:,0], tt)
plt.show()

이렇게하면 등록된 target중에서 distinct한 값만 반환해줌. 이번 데이터타겟은 string의 값들인데 자동으로 숫자로 변환해준다함. 
print(kn.classes_) #'Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish' = distinct values from fish_target 알파벳순서임ㅋ

print(kn.predict(test_scaled[:5])) #['Bream' 'Perch' 'Perch' 'Perch' 'Bream']..
처음다섯개의 예상값을 물었는데 숫자로 주는게아니라 영어로 다시 바꿔서 돌려줌.
# print(test_scaled[:5])#return 0~5th rows of test_scaled
근데 이렇게하면 그냥 숫자값을 줘버려..

proba = kn.predict_proba(test_scaled[:5]) 하면 각행별로 어떤 확률로 어떤클래스에 분류되는지 보여줌.
[[1.     0.     0.     0.     0.     0.     0.    ] 100%확률로 0th = 100% 확률로bream
 [0.     0.     0.6667 0.     0.3333 0.     0.    ] 66%확률로 Perch, 33%확률로 Roach!
 [0.     0.     0.6667 0.     0.3333 0.     0.    ]
 [0.     0.     1.     0.     0.     0.     0.    ]
 [0.3333 0.     0.3333 0.     0.     0.     0.3333]]

좀더 그럴싸한 확률얻는 방법은 로지스틱회귀라는게있따.

z= a* wei  + b*len + c*dia + d*hei + e*wid + f 이런식으로 사용한다고한다.
그대로사용하면 그냥 회귀가 되므로 [-oo, oo] 범위를 [0,1]로 바꿔줘야함

1/(1+e^-z)의 값이 0.5 초과이면 양성 , 0.5이하면 음성클래스라고한다.

그냥 이건 z의 값이 0보다 작으면 음성 0보다크면 양성이다. 왜냐면 e^-z = 1/e^z인데 z가 커질수록 이값은 0이되므로 1/(1+0) = 1 = 양성. 

어떤 array에서 특정값을 가진 index만 뽑아 내고 싶다면,,?
bream_smelt_indexes = (train_target =='Bream') | (train_target == 'Smelt')
# print(bream_smelt_indexes)
-> true, false로 이루어진 array 변환. bream이나 smelt면 true.

저기서 true라고 된 index만 추출하기. true라고 적힌 index = train_scaled에서 뽑을 row index
train_bream_smelt = train_scaled[bream_smelt_indexes]
# print(train_bream_smelt)
target_bream_smelt = train_target[bream_smelt_indexes]
# print(target_bream_smelt)

선형모델을 쓸때 LinearRegression쓰는거맹키로 로지스틱써도됨
from sklearn.linear_model import LogisticRegression #predict a value from linear equation with given input..
lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)
print(lr.predict(train_bream_smelt[:5])) #['Bream' 'Smelt' 'Bream' 'Bream' 'Bream']
어짜피 여기서는 bream이나 smelt밖에없기때문에 이거밖에나올 수가 없음.

print(lr.predict_proba(train_bream_smelt[:5])) # row별로 bream일확률 vs smelt일 확률

print(lr.coef_, lr.intercept_) #[[-0.41581543 -0.59429439 -0.68212298 -1.02045413 -0.76459742]] [-2.25744997] 이것으로 식의 계수, 절편을 알 수 있다.  

decisions = lr.decision_function(train_bream_smelt[:5]) #a* wei  + b*len + c*dia + d*hei + e*wid + f 

보통 양성값의 결과만 보여주잖아, 음성을 보는방법도있다
from scipy.special import expit #calculate probability of negative class

print(expit(decisions)) 음성 class일 확률을 보여준다.

다중분류에 대해 알아보자.

#multi regression
lr = LogisticRegression(C=20, max_iter= 1000) #max_iter: # of repeat. default is 100. As C increases, regulation decreases.
lr.fit(train_scaled, train_target)
C가 올라갈수록 규제 하락, C가 내려갈수록 규제 증가. max_iter는 반복할 횟수.

print(lr.score(train_scaled, train_target)) #0.9411764705882353
print(lr.score(test_scaled, test_target)) #0.875

proba = lr.predict_proba(test_scaled[:5])
print(np.round(proba, decimals = 3))
test_scaled은 input값을 scale맞춰준거임. 그것의 처음 5개의 row들을 어떤 확률로 뭐가나올지 검사하는거야. 그럼 확률의 배열이 반환돼.
round는 딱봐도 반올림인거알지.

print(lr.coef_.shape, lr.intercept_.shape) #(7, 5) where 7 = # of rows(classes), 5 = # of columns(coefs). (7,)
print(lr.coef_, lr.intercept_) #Where 7 is # of classes (Species, target), 5 is # of attributes(Weight, Length, Diagonal, Height, Width. input)
여기서 7과 5가나오는데, 7은 target array로 우리가 등록을 했잖아. (lr.fit(train_scaled, train_target) 맞지?) 그거의 distinct한 값의 수야. 여기서는 print(kn.classes_) #'Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish' 즉 7개의 값이있어. 그래서 7이고, 5는 input의 종류야. csv파일에서 fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy() 저 5개의 값들을 불러왔기때문에 총 5개야.

****coef, intercept가 주어지면 Z를 구할 수 있다. 그것으로 probability를 구할때 소프트/시그모 중 하나 쓰는듯? 아 시그모이드는 1/(1+e^(-z)) 엿는데 이걸로하면 모든 합이 1이안나온데, 대신 softMax라는 s1 = e^(z1)/(e_sum) s2= e^(z2)/(e_sum) ... 이렇게 구하라고하네..

e_sum = e^(z0) + e^(z1) ... + e^(zN)까지 다더한거임..ㄷㄷ 이제 이해했네.
당연히 predict_proba 값과 같음. 왜냐면 softmax는 coef, intercept, decision(Z), 그것을 softmax라는 함수를 이용해서 확률을 구하는거기때문에. predict_proba는 저걸 하나의 function으로 함축한거네

decision = lr.decision_function(test_scaled[:5])
print(np.round(decision, decimals= 2))

proba = softmax(decision, axis=1)
print(np.round(proba, decimals=3))   #calculate probability with softmax using Z values(decision)

이렇게하는건데 아랫코드랑 결과값이 똑같음
proba = lr.predict_proba(test_scaled[:5]) 
print("lr.predict_proba",np.round(proba, decimals = 3)) 

16. machineLeraning8 
점진적학습: 데이터가 계속들어올때 기존에있던것에 추가한후 다시 훈련을 한다. 서비스중인걸 중단하고 다시 업데이트
온라인학습(서비스중에 업데이트가능) -> 확률적경사하강법... 머라는지모르겟다. stochastic gradient descent SGD -> 최적화방법 . 무작위하게 경사를 내려가는 방법을 찾는법이라는 뜻으로 번역가능. 점진적으로 천천히 내려간다. 
샘플(확률적)하나씩 꺼내서 훈련(경사)시킨다->(반복)-> 훈련세트에 있는 샘플을 다썼다면-> 1에포크(epoch) 완료. 텅빈 훈련세트를 다시 채워서 2epoch 실행..반복
.. 하나씩 꺼내는거말고 여러개씩꺼내는건 미니배치경사하강법(2의배수.. 한번에 전부 꺼내기는 배치경사하강법 이라고한다.

손실함수: 머신러닝알고리즘이 얼마나 나쁜지 측정 (낮을수록 좋은 알고리즘)
미분가능한 함수로 해야한다.ㅋ-> 로지스틱 손실함수...
회귀에서는 평균 제곱, 절대값오차 함수를 사용가능(미분가능) 그래서 손실함수 = 측정지표
분류에서는 정확도로 성능을 측정, 로지스틱함수로 최적화측정

로지스틱손실함수. 예측확률:0~1의값.
-log0.2 와 -log0.8로 실험해보든지.
정답이 1일때:-log(예측확률)
값이 1에 근접할수록 작음.
값이 0에 근접할수록 큼

정답이 0일때:-log(1-예측확률) 
값이 0에 근접할수록 작음.
값이 1에 근접할수록 큼.

따라서 어떤 정답이든 예측값이 정답이랑 멀수록 값이 크다는것을 알 수 있다. 그래서 그값이 작은것을 찾아야 손실이작음. 크로스엔트로피손실함수라고도 부른다.

데이터전처리하는중 반드시!! 스케일을 맞춰줘야한다. 
에포크가 많아질수록 과대적합이되고 너무적으면 과소적합이라고한다. 

이건그냥 알아낸건데 DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel() 이런 애러가뜨면 저기서 시키는데로 마지막에 .ravel()붙이면되드라..

아무튼 이번에는 얼마나 학습하는게 가장 효율적이고 정확한지 알아볼거야
반드시 데이터를 전처리해서 분석해야해


from sklearn.linear_model import SGDClassifier #stochastic gradient descent. epoch epoch epoch. train train train.


sc = SGDClassifier(loss='log', max_iter=20, random_state=42) #max_iter = # of epoch (how many times repeat) max_iter는 반복할 횟수. random_state는 아무거나해도됨 회귀문제일때는 SGDRegressor을 쓰면된다. loss=log해줌으로써 로지스틱함수 쓰겠다는 뜻

#get accuracy 정확도측정
sc.fit(train_scaled, train_target.ravel())
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))


#train once more and get accuracy.
sc.partial_fit(train_scaled, train_target.ravel()) #to avoid DataConversionWarning, put .ravel()
print(sc.score(train_scaled, train_target)) 
print(sc.score(test_scaled, test_target))

그냥 fit이랑 pratial_fit이랑 다른점은 fit은 예전거 다버리고 다시 세팅하는반면, pratial_fit은 기존의 것을 유지하면서 점진적으로 추가로 한번 더 훈련할 수 있음

epoch가 많이할 수록 과소적합문제 (train < test) 발생하는데 너무적게하면 과대적합문제 (train > test)기 때문에 적당한 중앙값을 찾는 과정이 필요한데 그것을 '조기종료'라고한다.

sc = SGDClassifier(loss='log', random_state=42) 이렇게 max_iter를 정해주지않고 계속 돌린다.
classes = np.unique(train_target)
for _ in range(0, 300):
  sc.partial_fit(train_scaled, train_target.ravel(), classes = classes)
단, 이때 fit을 미리 해주지않았으므로 partial_fit 해줄때 classes를 언급해줄 필요가있다.

걍뭐대충 여기쯤이다라고 생각하고 epoch를 하나 정한다.

sc = SGDClassifier(loss ='log', max_iter =BEST EPOCH VALUE, tol=None, random_state=42)
sc.fit(train_scaled, train_target.ravel())
print(sc.score(train_scaled, train_target)) 
print(sc.score(test_scaled, test_target))

BEST EPOCH VALUE에 너가 생각하는 EPOCH값을 넣고 돌려본다. tol=None은 뭐 정밀도라는데.. 그냥써야하나.. 저걸 안쓰면 과대/과소가 뒤바껴..다른때는 몰라도 최적의 에포치를 넣는곳에서라도 써주자.

얘는 머신러닝알고리즘이아니라 머신러닝알고리즘을 최적화하는 방법임. epoch를 구하는 과정은 확률적경사하강법를 몇번해야 가장 정확한지를 알기위해 하는건가보다. 확률적경사하강법은 새로운 데이터가 들어왔을때 그것을 이용해서 다시 훈련하는거고..
