# -*- coding: utf-8 -*-
"""MachineLearning6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-T8QK-sQ8_0NBKYAz-yior8nFkyjp5Ha
"""

import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsRegressor
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
import pandas as pd
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
df = pd.read_csv('https://bit.ly/perch_csv')
# print(df)
perch_full = df.to_numpy()
# print(perch_full)
# print(perch_full[:,0])

#learn property of PolynomialFeatures.
# poly = PolynomialFeatures()
# poly.fit([[2,3]])
# print(poly.transform([[2,3]]))

perch_weight= [430.3,450.0,460.0,490.0,490.0,495.0,500.0,501.0,502.0,540.0,600.0,600.0,610.0,615.0,610.0,650.0,675.0,675.5,680.0,680.0,
               730.3,750.0,760.0,790.0,790.0,795.0,800.0,801.0,802.0,840.0,900.0,900.0,910.0,915.0,910.0,950.0,975.0,975.5,980.0,980.0,
               1030.3,1050.0,1060.0,1090.0,1090.0,1095.0,1100.0,1101.0,1102.0,1140.0,1200.0,1200.0,1210.0,1215.0,1210.0,1250.0
               ]

#we don't need stratify for regression.
#train, test input = perch_length, and train,test target = perch_weight
train_input, test_input, train_target, test_target = train_test_split(
    perch_full, perch_weight, random_state= len(perch_full)
)
train_input = np.array(train_input).reshape(-1,3)
test_input = np.array(test_input).reshape(-1,3)
#do this, otherwise we have typeerror: only integer scalar arrays can be converted to a scalar index site:stackoverflow.com
train_target = np.array(train_target).reshape(-1,1) 
test_target = np.array(test_target).reshape(-1,1)
# print(len(train_target)) #42
# print(len(train_input)) #42

poly = PolynomialFeatures(degree=2, include_bias= False)
poly.fit(train_input)
train_poly = poly.transform(train_input)

print("shape",train_poly.shape) #(42,9) when degree =2
print(poly.get_feature_names_out()) #['x0' 'x1' 'x2' 'x0^2' 'x0 x1' 'x0 x2' 'x1^2' 'x1 x2' 'x2^2']

test_poly = poly.transform(test_input)
# print(test_poly)

lr = LinearRegression()
lr.fit(train_poly, train_target)

print("LR score:",lr.score(train_poly, train_target))
print("LR score:",lr.score(test_poly, test_target))

ss = StandardScaler();
ss.fit(train_poly)

train_scaled = ss.transform(train_poly)
test_scaled = ss.transform(test_poly)

#Ridge Regression
ridge = Ridge(alpha=0) #default alpha =1
ridge.fit(train_scaled, train_target)

print("Ridge score",ridge.score(train_scaled, train_target))
print("Ridge score",ridge.score(test_scaled, test_target))

# #find Best alpha value 
# alpha_list = [0.00000001,0.0000001,0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000]
# train_score_list = []
# test_score_list = []
# for alpha in alpha_list:
#   ridge = Ridge(alpha=alpha)
  
#   ridge.fit(train_scaled, train_target)
#   train_score_list.append(ridge.score(train_scaled, train_target))
#   test_score_list.append(ridge.score(test_scaled, test_target))

# plt.plot(np.log10(alpha_list), train_score_list)
# plt.plot(np.log10(alpha_list), test_score_list)
# plt.show()
# print(np.log10(alpha_list))

#Lasso Regression
lasso = Lasso(alpha=10)
lasso.fit(train_scaled, train_target)

print("Lasso score",lasso.score(train_scaled, train_target))
print("Lasso score",lasso.score(test_scaled, test_target))
print("# of attributes using Lasso: ",np.sum(lasso.coef_==0))

# #find Best alpha value 
# alpha_list = [0.001,0.01,0.1,1,10,100,1000]
# train_score_list = []
# test_score_list = []
# for alpha in alpha_list:
#   lasso = Lasso(alpha= alpha)
  
#   lasso.fit(train_scaled, train_target)
#   train_score_list.append(lasso.score(train_scaled, train_target))
#   test_score_list.append(lasso.score(test_scaled, test_target))

# plt.plot(np.log10(alpha_list), train_score_list)
# plt.plot(np.log10(alpha_list), test_score_list)
# plt.show()

# unscaled
# plt.scatter(train_poly[:,0], train_target)
# plt.scatter(train_poly[:,1], train_target)
# plt.scatter(train_poly[:,2], train_target)
# plt.show()

plt.scatter(train_scaled[:,0], train_target)
plt.scatter(train_scaled[:,1], train_target)
plt.scatter(train_scaled[:,2], train_target)
plt.show()
l = 28.4
h = 7.11
w = 4.14
list = np.array([l,h,w]).reshape(-1,3)

poly.fit(list)
input = poly.transform(list)
print("input",input)

ss = StandardScaler();
ss.fit(input)
# input_scaled = ss.transform(input)

#cant get scaled input because there is only one row. The formular of scale is (value - mean)/std. 
#In this case, mean is equal to the only row. Therefore, return all zeros.

# print(input_scaled)

ridge = Ridge(alpha=1) #default alpha =1
ridge.fit(train_poly, train_target)
lasso = Lasso(alpha=100)
lasso.fit(train_poly, train_target)

print("LR predict",lr.predict(input))
print("Ridge predict",ridge.predict(input))
print("Lasso predict",lasso.predict(input))


plt.scatter(train_poly[:,0], train_target)
# predicted values
plt.scatter(input[:,0], lr.predict(input), marker='^')
plt.scatter(input[:,0],ridge.predict(input), marker='D' )
plt.scatter(input[:,0],lasso.predict(input), marker='D' )
plt.show()



