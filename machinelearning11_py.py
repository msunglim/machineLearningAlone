# -*- coding: utf-8 -*-
"""machineLearning11.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10VY0mHyRcAbHHrsiIVNU1nX4ksQU0ld7
"""

import matplotlib.pyplot as plt #draw graph
import numpy as np #make array
from sklearn.model_selection import train_test_split #split array for train/test set
import pandas as pd #use file from url
from sklearn.preprocessing import StandardScaler #make scaled array for multi variables
from sklearn.linear_model import LogisticRegression #predict a value from linear equation with given input..
from sklearn.linear_model import SGDClassifier #stochastic gradient descent. epoch epoch epoch. train train train.
from sklearn.tree import DecisionTreeClassifier #calculate decision tree probability
from sklearn.tree import plot_tree #used when drawing a graph of Decision Tree
from sklearn.model_selection import StratifiedKFold #divide a set by k folds. use for classification. just use KFold for regression
from sklearn.model_selection import GridSearchCV #use cross verification over and over again with different parameter
from scipy.stats import uniform, randint #generate random number for gridsearch params
from sklearn.model_selection import RandomizedSearchCV #grid search for randomly splited cross verification samples.

from sklearn.model_selection import cross_validate #cross verification which divides a set by 5 subsets for default and test all of them.
from sklearn.ensemble import RandomForestClassifier #using bootstrap, randomly use n attributes for sampling and repeat this 100 times for default. Sum of score of all sample/number of trees is the score of randomforest.
from sklearn.ensemble import ExtraTreesClassifier #not using bootstrap, randomly use n attributes to make the best score. very fast, less accurate than randomforest. To gain more accurate score, must increase the number of trees.
from sklearn.ensemble import GradientBoostingClassifier #like loss function, find the minimum value of loss function. very useful algorithm. 

from sklearn.ensemble import HistGradientBoostingClassifier #divide train set into 256 groups : one group for null attribute and 255 for others. fast!
from sklearn.inspection import permutation_importance #switch one attribute with another and repeat over and over again. Compare the score of switched data and score of the original in order to figure out which attribute is critical.
from xgboost import XGBClassifier #only for gradientboosting!
from lightgbm import LGBMClassifier #only for gradientboosting!

wine = pd.read_csv('https://bit.ly/wine_csv_data')
# print(wine.head()) return fisrt n rows where default of n is 5.
# wine.info() show csv column information: non null count, dtype
# wine.describe() show column information: count, mean, std, min, max, 25%,50%,75% value

wine_data = wine[['alcohol','sugar','pH']].to_numpy()
wine_target = wine[['class']].to_numpy()
train_input, test_input, train_target, test_target = train_test_split(
  wine_data, wine_target, test_size=0.2, random_state = 42    
)

print('Random forest')
rf = RandomForestClassifier(n_jobs=-1, random_state= 42)
scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs = -1)

print("train score:",np.mean(scores['train_score']), "test score:",np.mean(scores['test_score']))

rf.fit(train_input, train_target.ravel())
print("importances:",rf.feature_importances_)

rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state= 42)
rf.fit(train_input, train_target.ravel())
print("oob score:",rf.oob_score_)


print('Extra tree')
et = ExtraTreesClassifier(n_jobs= -1, random_state =42)
scores = cross_validate(et, train_input, train_target, return_train_score =True, n_jobs =-1)

print("train score:",np.mean(scores['train_score']), "test score:",np.mean(scores['test_score']))

et.fit(train_input, train_target.ravel())
print("importances:", et.feature_importances_)

print('GradientBoosting')

gb = GradientBoostingClassifier(random_state = 42)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs = -1)
print("tran score:", np.mean(scores['train_score']), "test score:",np.mean(scores['test_score']))

gb = GradientBoostingClassifier(random_state = 42, n_estimators=500, learning_rate=0.2)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs = -1)
print("train score:", np.mean(scores['train_score']), "test score:",np.mean(scores['test_score']))

gb.fit(train_input, train_target.ravel())
print('importances:', gb.feature_importances_)

print('Histogram gradient boosting')

hgb = HistGradientBoostingClassifier(random_state= 42)
scores = cross_validate(hgb, train_input, train_target, return_train_score= True, n_jobs = -1)
print("train score:", np.mean(scores['train_score']), "test score:",np.mean(scores['test_score']))

print('permutation importance for knowing importances of hgb')
hgb.fit(train_input, train_target.ravel())
result = permutation_importance(hgb, train_input, train_target, n_repeats= 10, random_state=42, n_jobs = -1)
print('importance:', result.importances_mean)

result = permutation_importance(hgb, test_input, test_target, n_repeats= 10, random_state=42, n_jobs = -1)
print('importance:', result.importances_mean)

print('hbg score:', hgb.score(test_input, test_target))

print('XGBoost')
xgb = XGBClassifier(tree_method='hist', random_state= 42)
scores = cross_validate(xgb, train_input, train_target, return_train_score=True, n_jobs = -1)
print("train score:",np.mean(scores['train_score']),"test score:", np.mean(scores['test_score']))

print('LightGBM')
lgb = LGBMClassifier(random_state=42)
scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs = -1)
print("train score:",np.mean(scores['train_score']),"test score:", np.mean(scores['test_score']))